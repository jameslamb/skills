{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jlamb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# yes it's a thing\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import argparse\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import Phrases\n",
    "from nltk.corpus import stopwords\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/jlamb/repos/w210/skills/sandbox/data_science/app_files/sample_resume.txt\", \"r\") as f:\n",
    "    user_input_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dict_file = \"models/trigram_dictionary.dict\"\n",
    "bigram_model = \"models/bigram_model_pos\"\n",
    "trigram_model = \"models/trigram_model_pos\"\n",
    "lda_model = \"models/lda_alpha_eta_auto_27\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary = Dictionary.load(trigram_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = Phrases.load(bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model = Phrases.load(trigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel.load(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "\n",
    "topic_names = {1: u'Consulting and Contracting',\n",
    "               2: u'DevOps',\n",
    "               3: u'* Meta Job Description Topic: Students and Education',\n",
    "               4: u'Finance and Risk',\n",
    "               5: u'* Meta Job Description Topic: Benefits',\n",
    "               6: u'* Meta Job Description Topic: Facebook Advertising',\n",
    "               7: u'Aerospace and Flight Technology',\n",
    "               8: u'* Meta Job Description Topic: Soft Skills',\n",
    "               9: u'Product Management',\n",
    "               10: u'Compliance and Process/Program Management',\n",
    "               11: u'Project and Program Management',\n",
    "               12: u'* Meta Job Description Topic: Generic',\n",
    "               13: u'* Meta Job Description Topic: EO and Disability',\n",
    "               14: u'Healthcare',\n",
    "               15: u'Software Engineering and QA',\n",
    "               16: u'Accounting and Finance',\n",
    "               17: u'Human Resources and People',\n",
    "               18: u'Sales',\n",
    "               19: u'* Meta Job Description Topic: Startup-Focused',\n",
    "               20: u'Federal Government and Defense Contracting',\n",
    "               21: u'Web Development and Front-End Software Engineering',\n",
    "               22: u'UX and Design',\n",
    "               23: u'* Meta Job Description Topic: Education-Focused',\n",
    "               24: u'Academic and Medical Research',\n",
    "               25: u'Data Science',\n",
    "               26: u'* Meta Job Description Topic: Non-Discrimination',\n",
    "               27: u'Business Strategy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hard_skills = []\n",
    "with open('models/hard_skills.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        all_hard_skills.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_input(input_doc, bigram_model, trigram_model, trigram_dictionary):\n",
    "    \"\"\"\n",
    "    (1) parse input doc with spaCy\n",
    "    (2) apply text pre-proccessing steps,\n",
    "    (3) create a bag-of-words representation\n",
    "    (4) create an LDA representation\n",
    "    \"\"\"\n",
    "\n",
    "    # parse the review text with spaCy\n",
    "    parsed_doc = nlp(input_doc)\n",
    "\n",
    "    # lemmatize the text and remove punctuation and whitespace\n",
    "    unigram_doc = []\n",
    "    for token in parsed_doc:\n",
    "        if not (token.is_punct or token.is_space):\n",
    "            unigram_doc.append(token.lemma)\n",
    "\n",
    "    # apply the first-order and secord-order phrase models\n",
    "    bigram_doc = bigram_model[unigram_doc]\n",
    "    trigram_doc = trigram_model[bigram_doc]\n",
    "\n",
    "    # remove any remaining stopwords\n",
    "    trigram_review = [term for term in trigram_doc\n",
    "                      if not term in stopwords]\n",
    "\n",
    "    # create a bag-of-words representation\n",
    "    doc_bow = trigram_dictionary.doc2bow(trigram_doc)\n",
    "\n",
    "    # create an LDA representation\n",
    "    document_lda = lda[doc_bow]\n",
    "    return trigram_review, document_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_match_list(document_lda, topic_names, num_terms=100):\n",
    "    '''\n",
    "    Take the above results and just save to a list of the top 500 terms in the topic.\n",
    "    Also return the user's highest probability topic, along with the probability itself.\n",
    "    '''\n",
    "    sorted_doc_lda = sorted(document_lda, key=lambda review_lda: -review_lda[1])\n",
    "    topic_number = sorted_doc_lda[0][0]\n",
    "    freq = sorted_doc_lda[0][1]\n",
    "    highest_probability_topic = topic_names[topic_number + 1]\n",
    "    top_topic_skills = []\n",
    "    for term, term_freq in lda.show_topic(topic_number, topn=num_terms):\n",
    "        top_topic_skills.append(term)\n",
    "    return top_topic_skills, highest_probability_topic, round(freq, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_skills(top_topic_skills, user_skills):\n",
    "    return [item for item in top_topic_skills if item in user_skills]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_common_skills(top_topic_skills, user_skills):\n",
    "    return [item for item in top_topic_skills if item not in user_skills]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skills(text_document, num_skills):\n",
    "    '''\n",
    "    Output the following objects in a Python dictionary:\n",
    "    1. top_topic: Name of user's highest-percentage-match topics (str)\n",
    "    2. match_percent: The percentage match (number between 0 and 1) (float)\n",
    "    3. skills_in_common: List of ALL skills user has in common with topic\n",
    "    4. skills_not_in_common: List of ALL skills user does NOT have in common with topic\n",
    "    5. hard_skills_in_common: List of HARD skills user has in common with topic\n",
    "    6. hard_skills_not_in_common: List of HARD skills user does NOT have in common with topic\n",
    "    '''\n",
    "    user_skills, my_lda = vectorize_input(text_document,\n",
    "                                          bigram_model,\n",
    "                                          trigram_model,\n",
    "                                          trigram_dictionary)\n",
    "    skills_list, top_topic, percent_match = top_match_list(my_lda,\n",
    "                                                           topic_names,\n",
    "                                                           num_terms=500)\n",
    "\n",
    "    hard_skills_list = [skill for skill in skills_list if skill in all_hard_skills]\n",
    "    output_dict = {\n",
    "      \"top_topic\": top_topic,\n",
    "      \"percent_match\": percent_match,\n",
    "      \"skills\": {\n",
    "          \"all\": {\n",
    "              \"has\": common_skills(skills_list, user_skills)[:num_skills],\n",
    "              \"missing\": non_common_skills(skills_list, user_skills)[:num_skills]\n",
    "          },\n",
    "          \"hard\": {\n",
    "              \"has\": common_skills(hard_skills_list, user_skills)[:num_skills],\n",
    "              \"missing\": non_common_skills(hard_skills_list, user_skills)[:num_skills]\n",
    "          }\n",
    "      }\n",
    "    }\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlamb/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f638fbfc965b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                           \u001b[0mbigram_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                           \u001b[0mtrigram_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                           trigram_dictionary)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-3083d76500a8>\u001b[0m in \u001b[0;36mvectorize_input\u001b[0;34m(input_doc, bigram_model, trigram_model, trigram_dictionary)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# apply the first-order and secord-order phrase models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbigram_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigram_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munigram_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrigram_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrigram_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbigram_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# remove any remaining stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter\u001b[0m  \u001b[0;31m# delimiter used for lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0mis_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_is_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;31m# if the input is an entire corpus (rather than a single sentence),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m_is_single\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mtemp_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mobj_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter\u001b[0m  \u001b[0;31m# delimiter used for lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0mis_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_is_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;31m# if the input is an entire corpus (rather than a single sentence),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m_is_single\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0man\u001b[0m \u001b[0miterable\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mobj_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mtemp_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "user_skills, my_lda = vectorize_input(user_input_text,\n",
    "                                          bigram_model,\n",
    "                                          trigram_model,\n",
    "                                          trigram_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.phrases.Phrases at 0x111916550>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_doc = nlp(user_input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_doc = []\n",
    "for token in parsed_doc:\n",
    "    if not (token.is_punct or token.is_space):\n",
    "        unigram_doc.append(token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlamb/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigram_doc = bigram_model[unigram_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x15f5a2518>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x15f5a2518>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlamb/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3570abe075a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrigram_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrigram_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbigram_doc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter\u001b[0m  \u001b[0;31m# delimiter used for lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0mis_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_is_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;31m# if the input is an entire corpus (rather than a single sentence),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m_is_single\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mtemp_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mobj_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter\u001b[0m  \u001b[0;31m# delimiter used for lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0mis_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_is_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;31m# if the input is an entire corpus (rather than a single sentence),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m_is_single\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0man\u001b[0m \u001b[0miterable\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mobj_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mtemp_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "trigram_doc = trigram_model[bigram_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (skills)",
   "language": "python",
   "name": "resume_parsing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
