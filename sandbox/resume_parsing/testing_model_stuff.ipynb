{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from nltk.corpus import stopwords\n",
    "import en_core_web_sm\n",
    "\n",
    "# yes it's a thing\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/jlamb/repos/w210/skills/sandbox/data_science/app_files/sample_resume.txt\", \"r\") as f:\n",
    "    user_input_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dict_file = \"models/trigram_dictionary.dict\"\n",
    "bigram_model_file = \"models/bigram_model_pos\"\n",
    "trigram_model_file = \"models/trigram_model_pos\"\n",
    "lda_model_file = \"models/lda_alpha_eta_auto_27\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary = Dictionary.load(trigram_dict_file)\n",
    "bigram_model = Phrases.load(bigram_model_file)\n",
    "trigram_model = Phrases.load(trigram_model_file)\n",
    "lda = LdaModel.load(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of hard skills\n",
    "all_hard_skills = []\n",
    "with open('models/hard_skills.txt', 'r') as infile:\n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        all_hard_skills.append(line)\n",
    "\n",
    "# Load globals\n",
    "nlp = spacy.load('en')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_names = {1: u'Consulting and Contracting',\n",
    "               2: u'DevOps',\n",
    "               3: u'* Meta Job Description Topic: Students and Education',\n",
    "               4: u'Finance and Risk',\n",
    "               5: u'* Meta Job Description Topic: Benefits',\n",
    "               6: u'* Meta Job Description Topic: Facebook Advertising',\n",
    "               7: u'Aerospace and Flight Technology',\n",
    "               8: u'* Meta Job Description Topic: Soft Skills',\n",
    "               9: u'Product Management',\n",
    "               10: u'Compliance and Process/Program Management',\n",
    "               11: u'Project and Program Management',\n",
    "               12: u'* Meta Job Description Topic: Generic',\n",
    "               13: u'* Meta Job Description Topic: EO and Disability',\n",
    "               14: u'Healthcare',\n",
    "               15: u'Software Engineering and QA',\n",
    "               16: u'Accounting and Finance',\n",
    "               17: u'Human Resources and People',\n",
    "               18: u'Sales',\n",
    "               19: u'* Meta Job Description Topic: Startup-Focused',\n",
    "               20: u'Federal Government and Defense Contracting',\n",
    "               21: u'Web Development and Front-End Software Engineering',\n",
    "               22: u'UX and Design',\n",
    "               23: u'* Meta Job Description Topic: Education-Focused',\n",
    "               24: u'Academic and Medical Research',\n",
    "               25: u'Data Science',\n",
    "               26: u'* Meta Job Description Topic: Non-Discrimination',\n",
    "               27: u'Business Strategy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_input(input_doc, bigram_model, trigram_model, trigram_dictionary):\n",
    "    \"\"\"\n",
    "    (1) parse input doc with spaCy\n",
    "    (2) apply text pre-proccessing steps,\n",
    "    (3) create a bag-of-words representation\n",
    "    (4) create an LDA representation\n",
    "    \"\"\"\n",
    "\n",
    "    # parse the review text with spaCy\n",
    "    parsed_doc = nlp(input_doc)\n",
    "\n",
    "    # lemmatize the text and remove punctuation and whitespace\n",
    "    unigram_doc = []\n",
    "    for token in parsed_doc:\n",
    "        if not (token.is_punct or token.is_space):\n",
    "            unigram_doc.append(token.lemma_)\n",
    "\n",
    "    # apply the first-order and secord-order phrase models\n",
    "    bigram_doc = bigram_model[unigram_doc]\n",
    "    trigram_doc = trigram_model[bigram_doc]\n",
    "\n",
    "    # remove any remaining stopwords\n",
    "    trigram_review = [term for term in trigram_doc\n",
    "                      if not term in stopwords]\n",
    "\n",
    "    # create a bag-of-words representation\n",
    "    doc_bow = trigram_dictionary.doc2bow(trigram_doc)\n",
    "\n",
    "    # create an LDA representation\n",
    "    document_lda = lda[doc_bow]\n",
    "    return trigram_review, document_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_doc = user_input_text\n",
    "parsed_doc = nlp(input_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlamb/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "user_skills, my_lda = vectorize_input(user_input_text,\n",
    "                                      bigram_model,\n",
    "                                      trigram_model,\n",
    "                                      trigram_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillRecommender:\n",
    "\n",
    "    def __init__(self,\n",
    "                 trigram_dictionary,\n",
    "                 bigram_model,\n",
    "                 trigram_model,\n",
    "                 lda_model,\n",
    "                 topic_names,\n",
    "                 hard_skills=[]):\n",
    "        \"\"\"\n",
    "        Create an instance of the thing we'll use to do skill\n",
    "        parsing and recommendation\n",
    "        \"\"\"\n",
    "        self.trigram_dictionary = trigram_dictionary\n",
    "        self.bigram_model = bigram_model\n",
    "        self.trigram_model = trigram_model\n",
    "        self.lda_model = lda_model\n",
    "        self.topic_names = topic_names\n",
    "        self.hard_skills = set(hard_skills)\n",
    "        self.nlp = spacy.load('en')\n",
    "\n",
    "    def fit(self, input_text):\n",
    "        \"\"\"\n",
    "        Given a string with an input document and the trained\n",
    "        bigram and trigram models, return a trigram document\n",
    "        representation.\n",
    "        \"\"\"\n",
    "        # parse the review text with spaCy\n",
    "        parsed_doc = self.nlp(input_text)\n",
    "\n",
    "        # lemmatize the text and remove punctuation and whitespace\n",
    "        unigram_doc = []\n",
    "        for token in parsed_doc:\n",
    "            if not (token.is_punct or token.is_space):\n",
    "                unigram_doc.append(token.lemma_)\n",
    "\n",
    "        # apply the first-order and secord-order phrase models\n",
    "        bigram_doc = self.bigram_model[unigram_doc]\n",
    "        trigram_doc = self.trigram_model[bigram_doc]\n",
    "\n",
    "        # Parse out skills\n",
    "        stopword_list = stopwords.words('english')\n",
    "        skills = filter(lambda x: x not in stopword_list, trigram_doc)\n",
    "\n",
    "        self.trigram_doc = trigram_doc\n",
    "        self.skills = set(skills)\n",
    "\n",
    "    def predict(self, skills_text, num_jobs=3, skills_per_job=10):\n",
    "        \"\"\"\n",
    "        Go do that hockey. Produces a JSON with an element called\n",
    "        \"predictions\" that holds a list of job area matches. Each job\n",
    "        has the following things:\n",
    "        1. \"job_name\"\n",
    "        2. \"match_percent\" = float in [0,1]. Match between user resume and job\n",
    "        3. \"skills\" = a dictionary with lists of skills\n",
    "        \"\"\"\n",
    "\n",
    "        # create a bag-of-words representation\n",
    "        doc_bow = self.trigram_dictionary.doc2bow(self.trigram_doc)\n",
    "\n",
    "        # create an LDA representation\n",
    "        document_lda = self.lda_model[doc_bow]\n",
    "\n",
    "        # sort topics in descending order by match probability\n",
    "        sorted_doc_lda = sorted(document_lda,\n",
    "                                key=lambda review_lda: -review_lda[1])\n",
    "\n",
    "        # Initialize a dictionary for predictions\n",
    "        preds = {\n",
    "            \"predictions\": []\n",
    "        }\n",
    "\n",
    "        # Update the dictionary of predictions\n",
    "        for i in range(num_jobs):\n",
    "            topic_number = sorted_doc_lda[i][0]\n",
    "\n",
    "            # get skills for this particular job\n",
    "            skills_with_freq = self.lda_model.show_topic(topic_number,\n",
    "                                                         topn=skills_per_job)\n",
    "\n",
    "            # Get just the list of skill names for this job\n",
    "            just_skills = set(map(lambda tup: tup[0], skills_with_freq))\n",
    "\n",
    "            # Get all the relevant skills designations\n",
    "            has = just_skills.intersection(self.skills)\n",
    "            missing = just_skills.difference(self.skills)\n",
    "\n",
    "            # Grab the relevant information to serve back\n",
    "            prediction = {\n",
    "                \"job_name\": self.topic_names[topic_number],\n",
    "                \"match_percent\": sorted_doc_lda[i][1],\n",
    "                \"skills\": {\n",
    "                    \"has\": {\n",
    "                        \"all\": list(has),\n",
    "                        \"labeled\": {\n",
    "                            \"hard\": list(has.intersection(self.hard_skills)),\n",
    "                            \"other\": list(has.difference(self.hard_skills))\n",
    "                        }\n",
    "                    },\n",
    "                    \"missing\": {\n",
    "                        \"all\": list(missing),\n",
    "                        \"labeled\": {\n",
    "                            \"hard\": list(missing.intersection(self.hard_skills)),\n",
    "                            \"other\": list(missing.difference(self.hard_skills))\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            preds[\"predictions\"].append(prediction)\n",
    "\n",
    "        return(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SkillRecommender\n",
    "model = SkillRecommender(\n",
    "    trigram_dictionary=Dictionary.load(trigram_dict_file),\n",
    "    bigram_model=Phrases.load(bigram_model_file),\n",
    "    trigram_model=Phrases.load(trigram_model_file),\n",
    "    lda_model=LdaModel.load(lda_model_file),\n",
    "    topic_names=topic_names,\n",
    "    hard_skills=all_hard_skills\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlamb/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# fit to input text\n",
    "model.fit(user_input_text)\n",
    "\n",
    "# get skills\n",
    "skills = model.skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(skills_text=\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-a6a3919e380f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{1: 'hey'}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "x = json.loads(\"{1: 'hey'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leadership',\n",
       " 'and',\n",
       " 'experience',\n",
       " 'head',\n",
       " 'of',\n",
       " 'data_science',\n",
       " 'shift',\n",
       " '|',\n",
       " 'berkeley',\n",
       " 'ca|',\n",
       " '2017',\n",
       " 'august',\n",
       " '2017',\n",
       " 'present',\n",
       " 'building',\n",
       " 'a',\n",
       " 'machine_learn',\n",
       " 'system',\n",
       " 'to',\n",
       " 'match',\n",
       " 'transition',\n",
       " 'military',\n",
       " 'service',\n",
       " 'member',\n",
       " 'to',\n",
       " 'tech',\n",
       " 'job',\n",
       " 'additional',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'growth',\n",
       " 'bi',\n",
       " 'datum',\n",
       " 'communication',\n",
       " 'and',\n",
       " 'evangelism',\n",
       " 'serve',\n",
       " 'as',\n",
       " 'pm',\n",
       " 'for',\n",
       " 'data',\n",
       " 'product',\n",
       " 'data_science',\n",
       " 'advisor',\n",
       " 'commonlit',\n",
       " '|',\n",
       " 'washington',\n",
       " 'd.c.',\n",
       " '|',\n",
       " 'september',\n",
       " '2017',\n",
       " 'present',\n",
       " 'advising',\n",
       " 'a',\n",
       " 'literacy',\n",
       " 'nonprofit',\n",
       " 'grow',\n",
       " 'at',\n",
       " '500k',\n",
       " 'user',\n",
       " 'month',\n",
       " 'to',\n",
       " 'be',\n",
       " 'data',\n",
       " 'drive',\n",
       " 'create',\n",
       " 'business',\n",
       " 'intelligence',\n",
       " 'workflow',\n",
       " 'and',\n",
       " 'teach',\n",
       " 'commonlit',\n",
       " 'employee',\n",
       " 'sql',\n",
       " 'a',\n",
       " 'b',\n",
       " 'testing',\n",
       " 'and',\n",
       " 'datum',\n",
       " 'visualization',\n",
       " 'implement',\n",
       " 'ml',\n",
       " 'psychometric',\n",
       " 'model',\n",
       " 'design',\n",
       " 'and',\n",
       " 'implement',\n",
       " 'a',\n",
       " 'complex',\n",
       " 'randomize',\n",
       " 'control',\n",
       " 'trial',\n",
       " 'that',\n",
       " 'successfully',\n",
       " 'test',\n",
       " 'the',\n",
       " 'effectiveness',\n",
       " 'of',\n",
       " 'one',\n",
       " 'of',\n",
       " 'commonlit',\n",
       " '’s',\n",
       " 'feature',\n",
       " 'balance',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'statistical',\n",
       " 'power',\n",
       " 'with',\n",
       " 'commonlit',\n",
       " '’s',\n",
       " 'high',\n",
       " 'standard',\n",
       " 'for',\n",
       " 'user',\n",
       " 'experience',\n",
       " 'data',\n",
       " 'architect',\n",
       " 'goldenkey',\n",
       " '|',\n",
       " 'san_francisco',\n",
       " '|',\n",
       " 'september',\n",
       " '2016-',\n",
       " 'may',\n",
       " '2017',\n",
       " 'hybrid',\n",
       " 'datum',\n",
       " 'architect',\n",
       " 'a',\n",
       " 'b',\n",
       " 'testing',\n",
       " 'lead',\n",
       " 'product',\n",
       " 'manager',\n",
       " 'business',\n",
       " 'intelligence',\n",
       " 'analyst',\n",
       " 'and',\n",
       " 'datum',\n",
       " 'product',\n",
       " 'builder',\n",
       " 'u.s.',\n",
       " 'army',\n",
       " 'infantry',\n",
       " 'officer',\n",
       " '101st',\n",
       " 'airborne',\n",
       " 'division',\n",
       " 'rakkasans',\n",
       " 'the',\n",
       " 'old',\n",
       " 'guard|',\n",
       " 'u.s.',\n",
       " 'afghanistan',\n",
       " '|',\n",
       " '2011',\n",
       " '16',\n",
       " 'plan',\n",
       " 'and',\n",
       " 'lead',\n",
       " '55',\n",
       " 'combat',\n",
       " 'mission',\n",
       " 'in',\n",
       " 'eastern',\n",
       " 'afghanistan',\n",
       " 'second',\n",
       " 'in',\n",
       " 'command',\n",
       " 'and',\n",
       " 'chief',\n",
       " 'of',\n",
       " 'staff',\n",
       " 'for',\n",
       " '99',\n",
       " 'soldier',\n",
       " 'and',\n",
       " '$_30',\n",
       " 'm',\n",
       " 'of',\n",
       " 'equipment',\n",
       " 'engineered',\n",
       " 'datum',\n",
       " 'collection',\n",
       " 'system',\n",
       " 'statistical_model',\n",
       " 'that',\n",
       " 'accurately',\n",
       " 'predict',\n",
       " '800-soldier',\n",
       " 'organization',\n",
       " \"'s\",\n",
       " 'top',\n",
       " 'kpi',\n",
       " 'six_month',\n",
       " 'into',\n",
       " 'the',\n",
       " 'future',\n",
       " 'innovated',\n",
       " 'method',\n",
       " 'to',\n",
       " 'airlift',\n",
       " '100',\n",
       " 'soldier',\n",
       " '22',\n",
       " 'vehicle',\n",
       " 'into',\n",
       " 'combat',\n",
       " 'with',\n",
       " '95',\n",
       " 'few',\n",
       " 'resource',\n",
       " 'consolidated',\n",
       " '$_57',\n",
       " 'm',\n",
       " 'of',\n",
       " 'equipment',\n",
       " 'spread',\n",
       " 'across',\n",
       " 'afghanistan',\n",
       " 'with',\n",
       " '100',\n",
       " 'accountability',\n",
       " 'skill',\n",
       " 'certifications',\n",
       " 'languages',\n",
       " 'python',\n",
       " 'sql',\n",
       " 'primary',\n",
       " 'r',\n",
       " 'for',\n",
       " 'statistic',\n",
       " 'and',\n",
       " 'psychometric',\n",
       " 'modeling',\n",
       " 'big_datum',\n",
       " 'mapreduce',\n",
       " 'use',\n",
       " 'spark_pyspark',\n",
       " 'sparksql',\n",
       " 'mrjob',\n",
       " 'and',\n",
       " 'hadoop',\n",
       " 'streaming',\n",
       " 'proficient',\n",
       " 'with',\n",
       " 'hive',\n",
       " 'familiar',\n",
       " 'with',\n",
       " 'storm',\n",
       " 'and',\n",
       " 'google_bigquery',\n",
       " 'experimentation',\n",
       " 'experience',\n",
       " 'designing_implement',\n",
       " 'hypothesis',\n",
       " 'test',\n",
       " 'for',\n",
       " 'causal_inference',\n",
       " 'span',\n",
       " 'from',\n",
       " 'simple',\n",
       " 'ui',\n",
       " 'optimization',\n",
       " 'to',\n",
       " 'complex',\n",
       " 'design',\n",
       " 'with',\n",
       " 'block',\n",
       " 'clustering',\n",
       " 'attrition',\n",
       " 'and',\n",
       " 'spillover',\n",
       " 'web',\n",
       " 'development',\n",
       " 'and',\n",
       " 'devops',\n",
       " 'proficient',\n",
       " 'with',\n",
       " 'html_css',\n",
       " 'and',\n",
       " 'basic',\n",
       " 'javascript',\n",
       " 'proficient',\n",
       " 'with',\n",
       " 'aws_ec2',\n",
       " 'emr_s3',\n",
       " 'heroku',\n",
       " 'and',\n",
       " 'gcc',\n",
       " 'familiar',\n",
       " 'with',\n",
       " 'rails',\n",
       " 'django',\n",
       " 'and',\n",
       " 'docker',\n",
       " 'additional',\n",
       " 'favorite',\n",
       " 'library',\n",
       " 'tool',\n",
       " 'scikit_learn',\n",
       " 'nltk',\n",
       " 'tensorflow',\n",
       " 'pandas',\n",
       " 'data.table',\n",
       " 'mllib',\n",
       " 'gensim',\n",
       " 'postgresql',\n",
       " 'google',\n",
       " 'analytics',\n",
       " 'hotjar',\n",
       " 'optimizely',\n",
       " 'visualization',\n",
       " 'highly_proficient',\n",
       " 'in',\n",
       " 'tableau',\n",
       " 'proficient',\n",
       " 'with',\n",
       " 'plotly',\n",
       " 'matplotlib',\n",
       " 'and',\n",
       " 'bokeh',\n",
       " 'proficient',\n",
       " 'with',\n",
       " 'network',\n",
       " 'visualization',\n",
       " 'use',\n",
       " 'gephi',\n",
       " 'and',\n",
       " 'networkx',\n",
       " 'familiar',\n",
       " 'with',\n",
       " 'd3.js',\n",
       " 'u.s.',\n",
       " 'army',\n",
       " 'airborne',\n",
       " 'ranger',\n",
       " 'certified',\n",
       " 'scrum_master',\n",
       " 'large_scale',\n",
       " 'scraping',\n",
       " 'capable',\n",
       " 'use',\n",
       " 'selenium',\n",
       " 'and',\n",
       " 'beautifulsoup',\n",
       " 'for',\n",
       " 'large_scale',\n",
       " 'headless',\n",
       " 'browser',\n",
       " 'web_scrap',\n",
       " 'education',\n",
       " 'uc_berkeley',\n",
       " 'school',\n",
       " 'of',\n",
       " 'information',\n",
       " '2017',\n",
       " '2018',\n",
       " 'master',\n",
       " 'of',\n",
       " 'information',\n",
       " 'and',\n",
       " 'data_science',\n",
       " 'united',\n",
       " 'states',\n",
       " 'military',\n",
       " 'academy',\n",
       " 'at',\n",
       " 'west_point',\n",
       " '2007',\n",
       " '2011',\n",
       " 'b.s.',\n",
       " 'in',\n",
       " 'international',\n",
       " 'relation',\n",
       " 'with',\n",
       " 'track',\n",
       " 'in',\n",
       " 'mechanical_engineering',\n",
       " 'graduate',\n",
       " 'top',\n",
       " '7',\n",
       " 'in',\n",
       " 'class',\n",
       " 'honor',\n",
       " 'thesis',\n",
       " 'hbx',\n",
       " '|',\n",
       " 'harvard',\n",
       " 'business',\n",
       " 'school',\n",
       " 'pass',\n",
       " 'with',\n",
       " 'honor',\n",
       " 'spring',\n",
       " '2015',\n",
       " 'ten_week',\n",
       " 'online',\n",
       " 'credential',\n",
       " 'of',\n",
       " 'readiness',\n",
       " 'with',\n",
       " 'course',\n",
       " 'in',\n",
       " 'business',\n",
       " 'analytic',\n",
       " 'economic',\n",
       " 'and',\n",
       " 'financial',\n",
       " 'accounting',\n",
       " 'georgetown',\n",
       " 'university',\n",
       " 'certificate',\n",
       " 'in',\n",
       " 'data_science',\n",
       " 'january',\n",
       " 'may',\n",
       " '2016',\n",
       " 'eight',\n",
       " 'course',\n",
       " 'on',\n",
       " 'campus',\n",
       " '108-hour',\n",
       " 'program',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'datum',\n",
       " 'science',\n",
       " 'pipeline',\n",
       " 'and',\n",
       " 'machine_learn',\n",
       " 'writing',\n",
       " 'and',\n",
       " 'project',\n",
       " 'skill',\n",
       " 'recommendation_engine',\n",
       " 'predict',\n",
       " 'crime',\n",
       " 'in',\n",
       " 'san_francisco',\n",
       " 'political',\n",
       " 'language',\n",
       " 'classifier',\n",
       " 'for',\n",
       " 'news_article',\n",
       " 'inc.',\n",
       " 'magazine',\n",
       " 'the',\n",
       " 'deep',\n",
       " 'pool',\n",
       " 'of',\n",
       " 'top_talent',\n",
       " '-PRON-',\n",
       " 'may',\n",
       " 'be',\n",
       " 'missing',\n",
       " '2016',\n",
       " 'washington_post',\n",
       " 'on',\n",
       " 'leadership',\n",
       " 'columnist',\n",
       " '2010',\n",
       " 'personal',\n",
       " 'avid',\n",
       " 'traveler',\n",
       " 'photographer',\n",
       " 'reader_writer',\n",
       " 'cook',\n",
       " 'tinkerer',\n",
       " 'and',\n",
       " 'beer',\n",
       " 'brewer',\n",
       " 'lifelong_learner']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trigram_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.corpora.dictionary.Dictionary"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trigram_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {\n",
    "    1: u'Consulting and Contracting',\n",
    "    2: u'DevOps',\n",
    "    3: u'* Meta Job Description Topic: Students and Education',\n",
    "    4: u'Finance and Risk',\n",
    "    5: u'* Meta Job Description Topic: Benefits',\n",
    "    6: u'* Meta Job Description Topic: Facebook Advertising',\n",
    "    7: u'Aerospace and Flight Technology',\n",
    "    8: u'* Meta Job Description Topic: Soft Skills',\n",
    "    9: u'Product Manager',\n",
    "    10: u'Compliance and Process/Program Management',\n",
    "    11: u'Project and Program Management',\n",
    "    12: u'* Meta Job Description Topic: Generic',\n",
    "    13: u'* Meta Job Description Topic: EO and Disability',\n",
    "    14: u'Healthcare',\n",
    "    15: u'Software Engineer',\n",
    "    16: u'Accounting and Finance',\n",
    "    17: u'Human Resources and People',\n",
    "    18: u'Sales',\n",
    "    19: u'* Meta Job Description Topic: Startup-Focused',\n",
    "    20: u'Federal Government and Defense Contracting',\n",
    "    21: u'Software Engineer',\n",
    "    22: u'UX Designer',\n",
    "    23: u'* Meta Job Description Topic: Education-Focused',\n",
    "    24: u'Academic and Medical Research',\n",
    "    25: u'Data Scientist',\n",
    "    26: u'* Meta Job Description Topic: Non-Discrimination',\n",
    "    27: u'Business Strategy'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(file=open(\"/Users/jlamb/repos/w210/skills/app/resume_parsing/models/topic_names.pkl\", \"wb\"), obj=topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pickle.load(open(\"/Users/jlamb/repos/w210/skills/app/resume_parsing/models/topic_names.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = json.loads('[1,2,3,4]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (skills)",
   "language": "python",
   "name": "resume_parsing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
