{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic is something like this:\n",
    "1. Identify the types of jobs we want to get the job postings for. Let us say one of them is \"backend engineer\". \n",
    "2. Use Selenium Webdriver to load the page http://www.indeed.com/m/jobs?q=backend+engineer (You can add other query parameters here like location, which we are ignoring here).Get the html and use BeautifulSoup to parse the content. This main search page tells us 3 important things - (a) The first 10 jobs for this search and their URLs, (b) The next to the next page of 10 jobs and (c) How many pages we need to click through to load all the jobs for this search.\n",
    "3. Write code to parse out the Job Urls in the first page. Every URL will be of this format: https://www.indeed.com/m/viewjob?jk=10bca72276277b22 . Load this page and get job title, location, salary, company name, summary and write one row per job into a CSV.\n",
    "4. Use selenium to click the next link to go to the next page and do exactly what we did in (3). Do these N times where N is the number of pages.\n",
    "5. Identify the next job type and repeat from (2).\n",
    "6. Upload all these CSVs into a S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend Engineer Jobs, Employment | Indeed Mobile\n"
     ]
    }
   ],
   "source": [
    "# This is a very simple example of how to parse stuff in the main Search page.\n",
    "start_url = \"http://www.indeed.com/m/jobs?q=backend+engineer\"\n",
    "page = requests.get(start_url)\n",
    "start_soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "print(start_soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('job title:', u'Senior SW Development Engineer')\n",
      "('company name:', u'Hitachi Vantara -')\n",
      "('location:', u'Santa Clara, CA')\n"
     ]
    }
   ],
   "source": [
    "# This is a very simple example of how to parse stuff out of an individual job page.\n",
    "test_url=\"https://www.indeed.com/m/viewjob?jk=10bca72276277b22\"\n",
    "test_job_link_page = requests.get(test_url)\n",
    "test_job_link_soup = BeautifulSoup(test_job_link_page.text, \"html.parser\")\n",
    "#print(test_job_link_soup.body.p.text)\n",
    "print('job title:', test_job_link_soup.body.p.b.text.strip())\n",
    "print('company name:', test_job_link_soup.body.p.b.next_sibling.next_sibling.string.strip())\n",
    "print('location:', test_job_link_soup.body.p.span.text.strip())\n",
    "#print('summary:', test_job_link_soup.find(name=\"div\", attrs={\"id\":\"desc\"}).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a soup object, parse out all the job urls.\n",
    "def extract_job_links(soup): \n",
    "  job_links = []\n",
    "  for h in soup.find_all(name=\"h2\", attrs={\"class\":\"jobTitle\"}):\n",
    "      for a in h.find_all(name=\"a\", attrs={\"rel\":\"nofollow\"}):\n",
    "        job_links.append(a[\"href\"])\n",
    "  return(job_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a list of job urls (links), parse out relevant content for all the job pages and store in a dataframe\n",
    "def extract_job_listings(job_links):\n",
    "    job_link_base_url=\"https://www.indeed.com/m/{}\"\n",
    "    job_listings=[]\n",
    "    for job_link in job_links:\n",
    "        j = random.randint(1000,2200)/1000.0\n",
    "        time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "        job_link_url = job_link_base_url.format(job_link)\n",
    "        #print('job_link_url:', job_link_url)\n",
    "        job_link_page = requests.get(job_link_url)\n",
    "        job_link_soup = BeautifulSoup(job_link_page.text, \"html.parser\")\n",
    "        #print('job_link_text:', job_link_soup.text)\n",
    "        #job_listings_df.loc[count] = extract_job_listing(job_link_url, job_link_soup)\n",
    "        job_listings.append(extract_job_listing(job_link_url, job_link_soup))\n",
    "    \n",
    "    \n",
    "    columns = [\"job_url\", \"job_title\", \"company_name\", \"location\", \"summary\", \"salary\"]\n",
    "    job_listings_df = pd.DataFrame(job_listings, columns=columns)\n",
    "    return job_listings_df\n",
    "\n",
    "# Given a single job listing url and the corresponding page, parse out the relevant content to create an entry \n",
    "def extract_job_listing(job_link_url, job_link_soup):\n",
    "    job_listing = []\n",
    "    job_listing.append(job_link_url)\n",
    "    job_listing.append(job_link_soup.body.p.b.text.strip())\n",
    "    job_listing.append(job_link_soup.body.p.b.next_sibling.next_sibling.string.strip())\n",
    "    job_listing.append(job_link_soup.body.p.span.text.strip())\n",
    "    job_listing.append(job_link_soup.find(name=\"div\", attrs={\"id\":\"desc\"}).text)\n",
    "    job_listing.append(\"Not_Found\")\n",
    "    return job_listing\n",
    "    \n",
    "    #print(job_listing)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a single page with many listings, go to the individual job pages and store all the content to CSV\n",
    "def parse_job_listings_to_csv(soup, fileName):\n",
    "    job_links = extract_job_links(soup)\n",
    "    job_posts = extract_job_listings(job_links)\n",
    "    job_posts.to_csv(fileName, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple example to show how to use Selenium to go through the next links.\n",
    "next_page_url_pattern=\"https://www.indeed.com/m/{}\"\n",
    "driver = webdriver.Chrome('/Users/Raghu/Downloads/chromedriver')\n",
    "start_url = \"http://www.indeed.com/m/jobs?q=data+analyst\"\n",
    "driver.set_page_load_timeout(15)\n",
    "driver.get(start_url)\n",
    "start_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "print('first page jobs:')\n",
    "print(extract_job_links(start_soup))\n",
    "#print start_soup.find(name='a', text='Next')\n",
    "next_link=driver.find_elements_by_xpath(\"//a[text()='Next']\")[0]\n",
    "#print next_link\n",
    "next_link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 1 page\n",
      "loading 2 page\n",
      "loading 3 page\n",
      "loading 4 page\n"
     ]
    }
   ],
   "source": [
    "# Use Selenium to go to do pagination - Click the next links (for now limit to only 5 next links)\n",
    "driver = webdriver.Chrome('/Users/Raghu/Downloads/chromedriver')\n",
    "start_url = \"http://www.indeed.com/m/jobs?q=frontend+engineer\"\n",
    "driver.set_page_load_timeout(15)\n",
    "driver.get(start_url)\n",
    "start_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "parse_job_listings_to_csv(start_soup, \"job_postings_0.csv\")\n",
    "\n",
    "for i in range(1,5):\n",
    "    print('loading {} page'.format(str(i)))\n",
    "    j = random.randint(1000,3300)/1000.0\n",
    "    time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "    next_page_url = driver.find_elements_by_xpath(\"//a[text()='Next']\")[0]\n",
    "    page_loaded = True\n",
    "    try:\n",
    "        next_page_url.click()\n",
    "    except TimeoutException:\n",
    "        get_info = False\n",
    "        driver.close()\n",
    "    if page_loaded:\n",
    "        soup=BeautifulSoup(driver.page_source)\n",
    "        parse_job_listings_to_csv(soup, \"job_postings_{}.csv\".format(str(i)))\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
